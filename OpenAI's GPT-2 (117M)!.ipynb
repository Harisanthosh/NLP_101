{"cells":[{"metadata":{"_uuid":"341ec2c324af64e043f793fb52d96fdb9f404821"},"cell_type":"markdown","source":"# Generate Text with OpenAI's GPT-2 Language Model (117M version)\n\n## For more information, refer to OpenAI's original blog post:\nhttps://blog.openai.com/better-language-models\n\n## Details:\nOpenAI only released their smaller 117M parameter model but even this small model performs suprisingly well. You can generate conditional samples from a given sentence or generate unconditional samples. \n\n### Tuning parameters for optimal predictions\nThe model starts repeating itself more often when given short prompts, but changing the temperature from the default of 0.7 can give you better results. Increasing the temperature forces the model to make more novel predictions, but often causes the model to go off topic. Decreasing the temperature keeps the model from going off topic, but causes the model to repeat itself more often. \n\n## Options:\n```\n--text : sentence to begin with.\n--quiet : not print all of the extraneous stuff like the \"================\"\n--nsamples : number of sample sampled in batch when multinomial function use\n--unconditional : If true, unconditional generation.\n--batch_size : number of batch size\n--length : sentence length (< number of context)\n--temperature: the thermodynamic temperature in distribution (default 0.7)\n--top_k : Returns the top k largest elements of the given input tensor along a given dimension. (default 40)\n```\n\n## Code:\n https://github.com/graykode/gpt-2-Pytorch"},{"metadata":{"trusted":true,"_uuid":"67405462faa6ce3378366d93fe3613309a3ac001","_kg_hide-output":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"# From https://github.com/graykode/gpt-2-Pytorch\nimport os\n!git clone https://github.com/graykode/gpt-2-Pytorch.git\nos.chdir('./gpt-2-Pytorch')\n!curl --output gpt2-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\n!pip install -r requirements.txt","execution_count":1,"outputs":[{"output_type":"stream","text":"Cloning into 'gpt-2-Pytorch'...\nremote: Enumerating objects: 1, done.\u001b[K\nremote: Counting objects: 100% (1/1), done.\u001b[K\nremote: Total 130 (delta 0), reused 0 (delta 0), pack-reused 129\u001b[K\nReceiving objects: 100% (130/130), 2.39 MiB | 1.03 MiB/s, done.\nResolving deltas: 100% (48/48), done.\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  522M  100  522M    0     0  13.9M      0  0:00:37  0:00:37 --:--:-- 16.4M\nCollecting regex==2017.4.5 (from -r requirements.txt (line 1))\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n\u001b[K     |████████████████████████████████| 604kB 2.7MB/s eta 0:00:01     |███████████████████▋            | 368kB 2.7MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: regex\n  Building wheel for regex (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533330 sha256=d6a882d9fa7a99b3d32d45e0d293902acc32a7519116eefe1e1d2351aaefd5d1\n  Stored in directory: /tmp/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\nSuccessfully built regex\n\u001b[31mERROR: allennlp 0.9.0 requires flaky, which is not installed.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 requires responses>=0.7, which is not installed.\u001b[0m\nInstalling collected packages: regex\n  Found existing installation: regex 2019.8.19\n    Uninstalling regex-2019.8.19:\n      Successfully uninstalled regex-2019.8.19\nSuccessfully installed regex-2017.4.5\n","name":"stdout"}]},{"metadata":{"_uuid":"110fa174bffecf1a33724b13b3ce273053177573"},"cell_type":"markdown","source":"# Generating conditional samples"},{"metadata":{"trusted":true,"_uuid":"cb06b3b1e1c2fbeeea255f652e4ddd31bc37dff8","scrolled":true},"cell_type":"code","source":"!python main.py --text \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"","execution_count":2,"outputs":[{"output_type":"stream","text":"Namespace(batch_size=-1, length=-1, nsamples=1, quiet=False, temperature=0.7, text='In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.', top_k=40, unconditional=False)\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n100%|█████████████████████████████████████████| 512/512 [00:07<00:00, 64.08it/s]\n======================================== SAMPLE 1 ========================================\n\n\n\"We were looking for the source of the word 'unicorn,' so we were talking about, 'It's not the unicorns' or 'It's the unicorns,'\" says Dr. Peter F. Krasnov, a professor of anthropology and head of the University of Montana's anthropology department.\n\nWhile it's not as common as it was in the past, the unicorns in the Andes Mountains are considered sacred by the indigenous people. The researchers believe that the unicorns are a cultural artifact that is being retained in the region to preserve the \"sacred past.\"\n\nThe unicorns were discovered in the Andes Mountains, and they have been found in the area for over 100 years.\n\n\"They are all very beautiful and they're quite beautiful,\" says Frans van der Walt, who was not involved in the research. \"I'm sure they are the most beautiful unicorns we've ever seen, and the most beautiful we've ever seen.\"\n\nThe researchers are currently searching for the source of the word 'unicorn' in the Andes Mountains.\n\nThe research is published in the journal Nature Communications.\n\nSource: University of Montana<|endoftext|>The number of children under 18 who are exposed to the chemicals in the home is rising as the number of children with autism, learning disability, and speech disorders increases.\n\nThe UK has the highest prevalence of the disorders in the world, and the number of children with autism is growing at an accelerating rate, according to the latest figures from the Child and Social Care Foundation.\n\nThe prevalence of autism has risen by nearly 6 per cent since 2011.\n\nThe children of children with autism are aged up to 25, and the prevalence of learning disabilities is on the rise, with a higher proportion of children with speech disorders in the UK than the rest of the world.\n\nAccording to figures from the report, the number of children with learning disabilities has grown by almost 5 per cent from 2011 to the previous year, and the number of children with learning disabilities has risen by more than 5 per cent from 2011 to the present.\n\nThe numbers are particularly striking as recent figures show that children with learning disability now make up half of all children in England and Wales.<|endoftext|>The UESPWiki – Your source for The Elder Scrolls since 1995\n\nThe following information is based on the lore of the game, which is in no way official source material. It is a work of fan-made artwork, and does not necessarily reflect the\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"ef9bbb30ca294bb4d4f0c6d2e3b2940e6d59b268","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"os.chdir('../')\n!rm -rf gpt-2-Pytorch","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}